{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nltk\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_buff = '../data/Buffers/'\n",
    "path_embd = '../GloVe/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = []\n",
    "for name in glob.glob(path_buff+'out_train*.txt'):\n",
    "    filenames.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/Buffers/out_train2.txt',\n",
       " '../data/Buffers/out_train3.txt',\n",
       " '../data/Buffers/out_train1.txt',\n",
       " '../data/Buffers/out_train0.txt',\n",
       " '../data/Buffers/out_train4.txt',\n",
       " '../data/Buffers/out_train5.txt',\n",
       " '../data/Buffers/out_train6.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_embd, 'r') as f:\n",
    "    wordlist = [line.split(None, 1)[0] for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Query Processing Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_query_unk = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "token_query = []\n",
    "def operations_on_query(data):\n",
    "    \n",
    "    global c\n",
    "    global token_query\n",
    "    global list_query_unk\n",
    "    \n",
    "    start_ = time.time()\n",
    "    query = data['query']\n",
    "    query = list(set(query)) # Because similar queries appears 10 times, makes processing faster\n",
    "    for i in range(len(query)):\n",
    "        token_q = nltk.word_tokenize(query[i])\n",
    "        for j in range(len(token_q)):\n",
    "            token_query.append(token_q[j])\n",
    "        token_query = list(set(token_query))\n",
    "    \n",
    "    for i in range(len(token_query)):\n",
    "        if token_query[i] not in wordlist:\n",
    "            list_query_unk.append(token_query[i])\n",
    "    end_ = time.time()\n",
    "    c+=1\n",
    "    print('Time elapsed for queries in Buffer {0} is {1}sec'.format(c, end_-start_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/Buffers/out_train2.txt\n",
      "Time elapsed for queries in Buffer 1 is 537.4920530319214sec\n",
      "../data/Buffers/out_train3.txt\n",
      "Time elapsed for queries in Buffer 2 is 1012.2576849460602sec\n",
      "../data/Buffers/out_train1.txt\n",
      "Time elapsed for queries in Buffer 3 is 1388.802297115326sec\n",
      "../data/Buffers/out_train0.txt\n",
      "Time elapsed for queries in Buffer 4 is 244.14059400558472sec\n",
      "../data/Buffers/out_train4.txt\n",
      "Time elapsed for queries in Buffer 5 is 2072.3488161563873sec\n",
      "../data/Buffers/out_train5.txt\n",
      "Time elapsed for queries in Buffer 6 is 2392.381336927414sec\n",
      "../data/Buffers/out_train6.txt\n",
      "Time elapsed for queries in Buffer 7 is 909.1734809875488sec\n",
      "Overall Time Elapsed in processing queries is 8590.890292167664s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    data = pd.read_csv(filenames[i], delimiter='\\t', names=['query_id', 'query', 'passage_text', 'label', 'passage_id'])\n",
    "    operations_on_query(data)\n",
    "end = time.time()\n",
    "print('Overall Time Elapsed in processing queries is {0}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique unk words in query:  155721\n"
     ]
    }
   ],
   "source": [
    "print('Total number of unique unk words in query: ', len(list_query_unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_query_unk = {'query_unk' : list_query_unk}\n",
    "df_query_unk = pd.DataFrame(dict_query_unk)\n",
    "df_query_unk.to_csv('tokens_query_unk.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Passage Processing Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_passage_unk = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "token_passage = []\n",
    "def operations_on_passage(data):\n",
    "    \n",
    "    global c\n",
    "    global token_passage\n",
    "    global list_passage_unk\n",
    "    \n",
    "    start_ = time.time()\n",
    "    passage = data['passage_text']\n",
    "    for i in range(len(passage)):\n",
    "        token_p = nltk.word_tokenize(passage[i])\n",
    "        for j in range(len(token_p)):\n",
    "            token_passage.append(token_p[j])\n",
    "        token_passage = list(set(token_passage))\n",
    "    for i in range(len(token_passage)):\n",
    "        if token_passage[i] not in wordlist:\n",
    "            list_passage_unk.append(token_passage[i])\n",
    "    end_ = time.time()\n",
    "    c+=1\n",
    "    print('Time elapsed for passages in Buffer {0} is {1}sec'.format(c, start_-end_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i in range(len(filenames)):\n",
    "    print(filenames[i])\n",
    "    data = pd.read_csv(filenames[i], delimiter='\\t', names=['query_id', 'query', 'passage_text', 'label', 'passage_id'])\n",
    "    operations_on_passage(data)\n",
    "end = time.time()\n",
    "print('Overall Time Elapsed in processing passages is {0}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Total number of unique unk words in passage: ', len(list_passage_unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
