{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNetwork(object):\n",
    "    '''\n",
    "    This is the stripped down version of Transformer network.\n",
    "\n",
    "    In MSAIC 2018 we have to select proper paragraphs with respect to the query passed. The idea is\n",
    "    attending to the important elements in query and passages and see the similarity in each one of\n",
    "    them and then decide which is appropriate one. Transformer network fits here perfectly as it\n",
    "    attends to both the query and passage and it's self attention picks the most important words.\n",
    "\n",
    "    The query vector obtained in multiple stages are then also fed into the passages and also\n",
    "    improves the fidelity of the outputs. We need to perform label smoothening due to disproportionate\n",
    "    distribution of nagative samples.\n",
    "\n",
    "    ========\n",
    "\n",
    "    [GOTO: https://stackoverflow.com/a/35688187]\n",
    "\n",
    "    The idea is that since we have an external embedding matrix, we can still use the\n",
    "    functionalities available in TF to use those embedding. This will require us to store the\n",
    "    embedding matrix in memory and then assign it at runtime. Function assign_embeddings() does it.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        scope,\n",
    "        model_name,\n",
    "        save_folder,\n",
    "        pad_id,\n",
    "        save_freq = 10,\n",
    "        is_training = True,\n",
    "        dim_model = 50,\n",
    "        ff_mid = 128,\n",
    "        ff_mid1 = 128,\n",
    "        ff_mid2 = 128,\n",
    "        num_stacks = 2,\n",
    "        num_heads = 5):\n",
    "        '''\n",
    "        Args:\n",
    "            scope: scope of graph\n",
    "            model_name: name for model\n",
    "            save_folder: folder for model saves\n",
    "            pad_id: integer of <PAD>\n",
    "            save_freq: frequency of saving\n",
    "            is_training: bool if network is in training mode\n",
    "            dim_model: same as embedding dimension\n",
    "            ff_mid: dimension in middle layer of inner feed forward network\n",
    "            ff_mid1: dimension in middle layer of outer feed forward network (L1)\n",
    "            ff_mid2: dimension in middle layer of outer feed forward network (L2)\n",
    "            num_stacks: number of stacks to use\n",
    "            num_heads: number of heads in SDPA\n",
    "\n",
    "        '''\n",
    "        self.scope = scope\n",
    "        self.model_name = model_name\n",
    "        self.is_training = is_training\n",
    "        self.save_folder = save_folder\n",
    "        self.save_freq = save_freq\n",
    "        self.pad_id = pad_id\n",
    "    \n",
    "        self.num_stacks = num_stacks\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_model = dim_model\n",
    "        self.ff_mid = ff_mid\n",
    "        self.ff_mid1 = ff_mid1\n",
    "        self.ff_mid2 = ff_mid2\n",
    "\n",
    "        self.global_step = 0\n",
    "\n",
    "\n",
    "    def build_model(self, emb, seqlen, batch_size = 32, print_stack = False):\n",
    "        '''\n",
    "        function to build the model end to end\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.seqlen = seqlen\n",
    "        self.print_stack = print_stack\n",
    "\n",
    "        with tf.variable_scope(self.scope):\n",
    "            # declaring the placeholders\n",
    "            self.query_input = tf.placeholder(tf.int32, [self.batch_size, self.seqlen], name = 'query_placeholder')\n",
    "            self.passage_input = tf.placeholder(tf.int32, [self.batch_size, self.seqlen], name = 'passage_placeholder')\n",
    "            self.target_input = tf.placeholder(tf.float32, [self.batch_size, 1], name = 'target_placeholder')\n",
    "            \n",
    "            # embedding matrix placeholder\n",
    "            self.embedding_matrix = tf.constant(emb, name = 'embedding_matrix', dtype = tf.float32)\n",
    "            \n",
    "            if self.print_stack:\n",
    "                print('[!] Building model...')\n",
    "                print('[*] self.query_input:', self.query_input)\n",
    "                print('[*] self.passage_input:', self.passage_input)\n",
    "                print('[*] self.target_input:', self.target_input)\n",
    "                print('[*] embedding_matrix:', self.embedding_matrix)\n",
    "\n",
    "            # now we need to add the padding in the computation graph\n",
    "            # masking\n",
    "            query_mask = self.construct_padding_mask(self.query_input)   \n",
    "            passage_mask = self.construct_padding_mask(self.passage_input)\n",
    "            \n",
    "            if self.print_stack:\n",
    "                print('[*] query_mask:', query_mask)\n",
    "                print('[*] passage_mask:', passage_mask)\n",
    "            \n",
    "            # lookup from embedding matrix\n",
    "            query_emb = self.get_embedding(self.embedding_matrix, self.query_input)\n",
    "            passage_emb = self.get_embedding(self.embedding_matrix, self.passage_input)\n",
    "            \n",
    "            if self.print_stack:\n",
    "                print('[*] query_emb:', query_emb)\n",
    "                print('[*] passage_emb:', passage_emb)\n",
    "            \n",
    "            # perform label smoothening on the labels\n",
    "            # label_smooth = self.label_smoothning(self.target_input)\n",
    "            label_smooth = self.target_input\n",
    "            \n",
    "            # model\n",
    "            q_out = query_emb\n",
    "            p_out = passage_emb\n",
    "            for i in range(self.num_stacks):\n",
    "                q_out = self.query_stack(q_in = q_out, mask = query_mask, scope = 'q_stk_{0}'.format(i))\n",
    "                if self.print_stack:\n",
    "                    print('[*] q_out ({0}):'.format(i), q_out)\n",
    "                p_out = self.passage_stack(p_in = p_out, q_out = q_out,\n",
    "                    query_mask = query_mask, passage_mask = passage_mask, scope = 'p_stk_{0}'.format(i))\n",
    "                if self.print_stack:\n",
    "                    print('[*] p_out ({0})'.format(i), p_out)\n",
    "\n",
    "            # now the custom part\n",
    "            ff_out = tf.layers.dense(p_out, self.ff_mid1, activation = tf.nn.relu) # (batch_size, seqlen, emb_dim)\n",
    "            ff_out = tf.layers.dense(ff_out, 1, activation = tf.nn.relu) # (batch_size, seqlen, 1)\n",
    "            ff_out_reshaped = tf.reshape(ff_out, [-1, seqlen]) # (batch_size, seqlen)\n",
    "            self.pred = tf.layers.dense(ff_out_reshaped, 1) # (batch_size, 1)\n",
    "                \n",
    "            if not self.is_training:\n",
    "                self.pred = tf.sigmoid(self.pred) # (batch_size, 1)\n",
    "                \n",
    "            if self.print_stack:\n",
    "                print('[*] predictions:', self.pred)\n",
    "\n",
    "            # loss and accuracy\n",
    "            self._accuracy = tf.reduce_sum(\n",
    "                tf.cast(tf.equal(self.pred, self.target_input), tf.float32)\n",
    "                ) / self.batch_size\n",
    "\n",
    "            self._loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(labels = label_smooth, logits = self.pred)\n",
    "                )\n",
    "\n",
    "            optim = tf.train.AdamOptimizer(beta1 = 0.9, beta2 = 0.98, epsilon = 1e-9)\n",
    "            self._train = optim.minimize(self._loss)\n",
    "            \n",
    "            if self.print_stack:\n",
    "                print('[*] accuracy:', self._accuracy)\n",
    "                print('[*] loss:', self._loss)\n",
    "                print('... Done!')\n",
    "\n",
    "        with tf.variable_scope(self.model_name + \"_summary\"):\n",
    "            tf.summary.scalar(\"loss\", self._loss)\n",
    "            tf.summary.scalar(\"accuracy\", self._accuracy)\n",
    "            self.merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    '''\n",
    "    NETWORK FUNCTIONS\n",
    "    =================\n",
    "\n",
    "    Following functions were placed outside this file with an aim to increase the\n",
    "    code value but is causing several issues, especially with the config file\n",
    "    redundancy. So putting them here and increasing the model simplicity but \n",
    "    complicating the codebase.\n",
    "    '''\n",
    "\n",
    "    ##### OPERATIONAL LAYERS #####\n",
    "\n",
    "    def get_embedding(self, emb, inp):\n",
    "        '''\n",
    "        get embeddings\n",
    "        '''\n",
    "        return tf.nn.embedding_lookup(emb, inp)\n",
    "\n",
    "    ##### CORE LAYERS #####\n",
    "\n",
    "    def sdpa(self, Q, K, V, mask = None):\n",
    "        '''\n",
    "        Scaled Dot Product Attention\n",
    "        q_size = k_size = v_size\n",
    "        Args:\n",
    "            Q:    (num_heads * batch_size, q_size, d_model)\n",
    "            K:    (num_heads * batch_size, k_size, d_model)\n",
    "            V:    (num_heads * batch_size, v_size, d_model)\n",
    "            mask: (num_heads * batch_size, q_size, d_model)\n",
    "        '''\n",
    "\n",
    "        qkt = tf.matmul(Q, tf.transpose(K, [0, 2, 1]))\n",
    "        qkt /= tf.sqrt(float(self.dim_model // self.num_heads))\n",
    "\n",
    "        # perform masking\n",
    "        qkt = tf.multiply(qkt, mask) + (1.0 - mask) * (-1e10)\n",
    "\n",
    "        soft = tf.nn.softmax(qkt) # (num_heads * batch_size, q_size, k_size)\n",
    "        soft = tf.layers.dropout(soft, training = self.is_training)\n",
    "        out = tf.matmul(soft, V) # (num_heads * batch_size, q_size, d_model)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def multihead_attention(self, query, key, value, mask = None, scope = 'attn'):\n",
    "        '''\n",
    "        Multihead attention with masking option\n",
    "        q_size = k_size = v_size = d_model/num_heads\n",
    "        Args:\n",
    "            query: (batch_size, q_size, d_model)\n",
    "            key:   (batch_size, k_size, d_model)\n",
    "            value: (batch_size, v_size, d_model)\n",
    "            mask:  (batch_size, q_size, d_model)\n",
    "        '''\n",
    "        with tf.variable_scope(scope):\n",
    "            # linear projection blocks\n",
    "            # print(query)\n",
    "            Q = tf.layers.dense(query, self.dim_model, activation = tf.nn.relu)\n",
    "            K = tf.layers.dense(key, self.dim_model, activation = tf.nn.relu)\n",
    "            V = tf.layers.dense(value, self.dim_model, activation = tf.nn.relu)\n",
    "\n",
    "            # split the matrix into multiple heads and then concatenate them to get\n",
    "            # a larger batch size: (num_heads, q_size, d_model/nume_heads)\n",
    "            Q_reshaped = tf.concat(tf.split(Q, self.num_heads, axis = 2), axis = 0)\n",
    "            K_reshaped = tf.concat(tf.split(K, self.num_heads, axis = 2), axis = 0)\n",
    "            V_reshaped = tf.concat(tf.split(V, self.num_heads, axis = 2), axis = 0)\n",
    "            mask = tf.tile(mask, [self.num_heads, 1, 1])\n",
    "\n",
    "            # scaled dot product attention\n",
    "            sdpa_out = self.sdpa(Q_reshaped, K_reshaped, V_reshaped, mask)\n",
    "            out = tf.concat(tf.split(sdpa_out, self.num_heads, axis = 0), axis = 2)\n",
    "\n",
    "            # final linear layer\n",
    "            out_linear = tf.layers.dense(out, self.dim_model)\n",
    "            out_linear = tf.layers.dropout(out_linear, training = self.is_training)\n",
    "\n",
    "        return out_linear\n",
    "\n",
    "    def feed_forward(self, x, scope = 'ff'):\n",
    "        '''\n",
    "        Position-wise feed forward network, applied to each position seperately\n",
    "        and identically. Can be implemented as follows\n",
    "        '''\n",
    "        with tf.variable_scope(scope):\n",
    "            out = tf.layers.conv1d(x, filters = self.ff_mid, kernel_size = 1,\n",
    "                activation = tf.nn.relu)\n",
    "            out = tf.layers.conv1d(out, filters = self.dim_model, kernel_size = 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def layer_norm(self, x):\n",
    "        '''\n",
    "        perform layer normalisation\n",
    "        '''\n",
    "        out = tf.contrib.layers.layer_norm(x, center = True, scale = True)\n",
    "        return out\n",
    "\n",
    "    def label_smoothning(self, x):\n",
    "        '''\n",
    "        perform label smoothning on the input label\n",
    "        '''\n",
    "        smoothed = (1.0 - self.ls_epsilon) * x + (self.ls_epsilon / vocab_size)\n",
    "        return smoothed\n",
    "\n",
    "    ###### STACKS ######\n",
    "\n",
    "    def query_stack(self, q_in, mask, scope):\n",
    "        '''\n",
    "        Single query stack \n",
    "        Args:\n",
    "            q_in: (batch_size, seqlen, embed_size)\n",
    "            mask: (batch_size, seqlen, seqlen)\n",
    "        '''\n",
    "        with tf.variable_scope(scope):\n",
    "            out = self.layer_norm(q_in + self.multihead_attention(q_in, q_in, q_in, mask))\n",
    "            out = self.layer_norm(out + self.feed_forward(out))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def passage_stack(self, p_in, q_out, query_mask, passage_mask, scope):\n",
    "        '''\n",
    "        Single passage stack\n",
    "        Args:\n",
    "            p_in: (batch_size, seqlen, embed_size)\n",
    "            q_out: output from query stack\n",
    "        '''\n",
    "        with tf.variable_scope(scope):\n",
    "            out = self.layer_norm(p_in + self.multihead_attention(p_in, p_in, p_in, mask = passage_mask))\n",
    "            out = self.layer_norm(out + self.multihead_attention(out, out, q_out, mask = query_mask, scope = 'attn2'))\n",
    "            out = self.layer_norm(out + self.feed_forward(out))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def construct_padding_mask(self, inp):\n",
    "        '''\n",
    "        Args:\n",
    "            inp: Original input of word ids, shape: [batch_size, seqlen]\n",
    "        Returns:\n",
    "            a mask of shape [batch_size, seqlen, seqlen] where <pad> is 0 and others are 1\n",
    "        '''\n",
    "        seqlen = inp.shape.as_list()[1]\n",
    "        mask = tf.cast(tf.not_equal(inp, self.pad_id), tf.float32)\n",
    "        mask = tf.tile(tf.expand_dims(mask, 1), [1, seqlen, 1])\n",
    "        return mask\n",
    "    \n",
    "    def print_network(self):\n",
    "        '''\n",
    "        Print the network in terms of \n",
    "        '''\n",
    "        network_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = self.scope)\n",
    "        for x in network_variables:\n",
    "            print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'trial1/q_stk_0/attn/dense/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/attn/dense/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/attn/dense_1/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/attn/dense_1/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/attn/dense_2/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/attn/dense_2/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/attn/dense_3/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/attn/dense_3/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/LayerNorm/beta:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/LayerNorm/gamma:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/ff/conv1d/kernel:0' shape=(1, 50, 128) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/ff/conv1d/bias:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/ff/conv1d_1/kernel:0' shape=(1, 128, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/ff/conv1d_1/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/LayerNorm_1/beta:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_0/LayerNorm_1/gamma:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn/dense/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn/dense/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn/dense_1/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn/dense_1/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn/dense_2/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn/dense_2/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn/dense_3/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn/dense_3/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/LayerNorm/beta:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/LayerNorm/gamma:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn2/dense/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn2/dense/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn2/dense_1/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn2/dense_1/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn2/dense_2/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn2/dense_2/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn2/dense_3/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/attn2/dense_3/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/LayerNorm_1/beta:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/LayerNorm_1/gamma:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/ff/conv1d/kernel:0' shape=(1, 50, 128) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/ff/conv1d/bias:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/ff/conv1d_1/kernel:0' shape=(1, 128, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/ff/conv1d_1/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/LayerNorm_2/beta:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_0/LayerNorm_2/gamma:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/attn/dense/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/attn/dense/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/attn/dense_1/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/attn/dense_1/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/attn/dense_2/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/attn/dense_2/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/attn/dense_3/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/attn/dense_3/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/LayerNorm/beta:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/LayerNorm/gamma:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/ff/conv1d/kernel:0' shape=(1, 50, 128) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/ff/conv1d/bias:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/ff/conv1d_1/kernel:0' shape=(1, 128, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/ff/conv1d_1/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/LayerNorm_1/beta:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/q_stk_1/LayerNorm_1/gamma:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn/dense/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn/dense/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn/dense_1/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn/dense_1/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn/dense_2/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn/dense_2/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn/dense_3/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn/dense_3/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/LayerNorm/beta:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/LayerNorm/gamma:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn2/dense/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn2/dense/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn2/dense_1/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn2/dense_1/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn2/dense_2/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn2/dense_2/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn2/dense_3/kernel:0' shape=(50, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/attn2/dense_3/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/LayerNorm_1/beta:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/LayerNorm_1/gamma:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/ff/conv1d/kernel:0' shape=(1, 50, 128) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/ff/conv1d/bias:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/ff/conv1d_1/kernel:0' shape=(1, 128, 50) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/ff/conv1d_1/bias:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/LayerNorm_2/beta:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/p_stk_1/LayerNorm_2/gamma:0' shape=(50,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/dense/kernel:0' shape=(50, 128) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/dense/bias:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/dense_1/kernel:0' shape=(128, 1) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/dense_2/kernel:0' shape=(12, 1) dtype=float32_ref>\n",
      "<tf.Variable 'trial1/dense_2/bias:0' shape=(1,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "network = TransformerNetwork(scope = 'trial1',\n",
    "                             model_name = 'trial1',\n",
    "                             save_folder = './trial',\n",
    "                             pad_id = int(12))\n",
    "\n",
    "# embeddings\n",
    "emb = np.random.rand(12, 50)\n",
    "\n",
    "# build network\n",
    "network.build_model(emb = emb, seqlen = 12, print_stack = False)\n",
    "network.print_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [32, 50], name = 'test_var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'split:0' shape=(32, 10) dtype=float32>, <tf.Tensor 'split:1' shape=(32, 10) dtype=float32>, <tf.Tensor 'split:2' shape=(32, 10) dtype=float32>, <tf.Tensor 'split:3' shape=(32, 10) dtype=float32>, <tf.Tensor 'split:4' shape=(32, 10) dtype=float32>]\n",
      "Tensor(\"concat:0\", shape=(160, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_reshape = tf.split(x, 5, axis = 1)\n",
    "print(x_reshape)\n",
    "x_reshape = tf.concat(x_reshape, axis = 0)\n",
    "print(x_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
